{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Training Metadata\n",
    "\n",
    "We can't train a model without a lot of data. Keeping track of where that data is and how to get it can be difficult.\n",
    "`rubicon` isn't in the business of storing full training datasets, but it can store metadata about our training datasets on both **projects** (for high level datasource configuration) and **experiments** (for indiviual model runs). \n",
    "\n",
    "Below, we'll use `rubicon` to reference a dataset stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.130486Z",
     "iopub.status.busy": "2021-03-05T15:28:31.129011Z",
     "iopub.status.idle": "2021-03-05T15:28:31.131048Z",
     "shell.execute_reply": "2021-03-05T15:28:31.132688Z"
    }
   },
   "outputs": [],
   "source": [
    "s3_config = {\n",
    "    \"region_name\": \"us-west-2\",\n",
    "    \"signature_version\": \"v4\",\n",
    "    \"retries\": {\n",
    "        \"max_attempts\": 10,\n",
    "        \"mode\": \"standard\",\n",
    "    }\n",
    "}\n",
    "\n",
    "bucket_name = \"my-bucket\"\n",
    "key = \"path/to/my/data.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the following function to pull training data locally from S3.\n",
    "\n",
    "**Note:** We're reading the user's account credentials from an external source rather than exposing them in the `s3_config` we created. `rubicon` is **not** intended for storing secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.139312Z",
     "iopub.status.busy": "2021-03-05T15:28:31.138120Z",
     "iopub.status.idle": "2021-03-05T15:28:31.141356Z",
     "shell.execute_reply": "2021-03-05T15:28:31.141877Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_from_s3(config, bucket, key, local_output_path):\n",
    "    import boto3\n",
    "    from botocore.config import Config\n",
    "    \n",
    "    config = Config(**config)\n",
    "\n",
    "    # assuming credentials are correct in `~/.aws` or set in environment variables\n",
    "    client = boto3.client(\"s3\", config=config)\n",
    "\n",
    "    with open(local_output_path, \"wb\") as f:\n",
    "        s3.download_fileobj(bucket, key, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't actually need to reach out to S3 for this example, so we'll use a no-op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.146170Z",
     "iopub.status.busy": "2021-03-05T15:28:31.145099Z",
     "iopub.status.idle": "2021-03-05T15:28:31.146733Z",
     "shell.execute_reply": "2021-03-05T15:28:31.147241Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_from_s3(config, bucket, key, local_output_path):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a **project** for the **experiments** we'll run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.151590Z",
     "iopub.status.busy": "2021-03-05T15:28:31.151082Z",
     "iopub.status.idle": "2021-03-05T15:28:31.858259Z",
     "shell.execute_reply": "2021-03-05T15:28:31.858983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project(name='Storing Training Metadata', id='78031671-e35b-4573-ac78-db4f0b2f6406', description=None, github_url=None, training_metadata=None, created_at=datetime.datetime(2021, 3, 5, 15, 28, 31, 852073))\n"
     ]
    }
   ],
   "source": [
    "from rubicon import Rubicon\n",
    "\n",
    "rubicon = Rubicon(persistence=\"memory\")\n",
    "project = rubicon.get_or_create_project(\"Storing Training Metadata\")\n",
    "\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create an **experiment**, we'll construct some training metadata to pass along so future collaborators, reviewers, or even future us can reference the same training dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.864265Z",
     "iopub.status.busy": "2021-03-05T15:28:31.863746Z",
     "iopub.status.idle": "2021-03-05T15:28:31.866499Z",
     "shell.execute_reply": "2021-03-05T15:28:31.866916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment(project_name='Storing Training Metadata', id='33be6771-4003-4d7c-aaff-152f6659f066', name=None, description=None, model_name=None, branch_name=None, commit_hash=None, training_metadata=[({'region_name': 'us-west-2', 'signature_version': 'v4', 'retries': {'max_attempts': 10, 'mode': 'standard'}}, 'my-bucket', 'path/to/my/data.parquet')], tags=['S3', 'training metadata'], created_at=datetime.datetime(2021, 3, 5, 15, 28, 31, 861872))\n"
     ]
    }
   ],
   "source": [
    "training_metadata = (s3_config, bucket_name, key)\n",
    "\n",
    "experiment = project.log_experiment(\n",
    "    training_metadata=training_metadata,\n",
    "    tags=[\"S3\", \"training metadata\"]\n",
    ")\n",
    "# then run the experiment and log everything to rubicon!\n",
    "\n",
    "print(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can come back any time and use the **experiment's** training metadata to pull the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.873750Z",
     "iopub.status.busy": "2021-03-05T15:28:31.873002Z",
     "iopub.status.idle": "2021-03-05T15:28:31.876743Z",
     "shell.execute_reply": "2021-03-05T15:28:31.876297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'region_name': 'us-west-2', 'signature_version': 'v4', 'retries': {'max_attempts': 10, 'mode': 'standard'}}, 'my-bucket', 'path/to/my/data.parquet')\n"
     ]
    }
   ],
   "source": [
    "experiment = project.experiments(tags=[\"S3\", \"training metadata\"], qtype=\"and\")[0]\n",
    "\n",
    "training_metadata = experiment.training_metadata\n",
    "print(training_metadata)\n",
    "\n",
    "read_from_s3(training_metadata[0], training_metadata[1], training_metadata[2], \"./data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're referencing multiple keys within the bucket, we can send a list of training metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.883464Z",
     "iopub.status.busy": "2021-03-05T15:28:31.882917Z",
     "iopub.status.idle": "2021-03-05T15:28:31.888887Z",
     "shell.execute_reply": "2021-03-05T15:28:31.889307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'region_name': 'us-west-2', 'signature_version': 'v4', 'retries': {'max_attempts': 10, 'mode': 'standard'}}, 'my-bucket', 'path/to/my/data_0.parquet')\n",
      "({'region_name': 'us-west-2', 'signature_version': 'v4', 'retries': {'max_attempts': 10, 'mode': 'standard'}}, 'my-bucket', 'path/to/my/data_1.parquet')\n",
      "({'region_name': 'us-west-2', 'signature_version': 'v4', 'retries': {'max_attempts': 10, 'mode': 'standard'}}, 'my-bucket', 'path/to/my/data_2.parquet')\n"
     ]
    }
   ],
   "source": [
    "training_metadata = [\n",
    "    (s3_config, bucket_name, \"path/to/my/data_0.parquet\"),\n",
    "    (s3_config, bucket_name, \"path/to/my/data_1.parquet\"),\n",
    "    (s3_config, bucket_name, \"path/to/my/data_2.parquet\"),\n",
    "]\n",
    "\n",
    "experiment = project.log_experiment(training_metadata=training_metadata)\n",
    "for training_metadata in experiment.training_metadata:\n",
    "    print(training_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`training_metadata` is simply a tuple or an array of tuples, so we can decide how to best store our metadata. The config and prefix are the same for each piece of metadata, so no need to duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.894223Z",
     "iopub.status.busy": "2021-03-05T15:28:31.893702Z",
     "iopub.status.idle": "2021-03-05T15:28:31.896797Z",
     "shell.execute_reply": "2021-03-05T15:28:31.896344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'region_name': 'us-west-2', 'signature_version': 'v4', 'retries': {'max_attempts': 10, 'mode': 'standard'}}, 'my-bucket', ['path/to/my/data_0.parquet', 'path/to/my/data_1.parquet', 'path/to/my/data_2.parquet'])\n"
     ]
    }
   ],
   "source": [
    "training_metadata = (\n",
    "    s3_config, bucket_name,\n",
    "    [\"path/to/my/data_0.parquet\", \"path/to/my/data_1.parquet\", \"path/to/my/data_2.parquet\"]\n",
    ")\n",
    "\n",
    "experiment = project.log_experiment(training_metadata=training_metadata)\n",
    "print(experiment.training_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's just an array of tuples, we can even use a `namedtuple` to represent the structure we decide to go with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.905113Z",
     "iopub.status.busy": "2021-03-05T15:28:31.903886Z",
     "iopub.status.idle": "2021-03-05T15:28:31.906387Z",
     "shell.execute_reply": "2021-03-05T15:28:31.906910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3TrainingMetadata(config={'region_name': 'us-west-2', 'signature_version': 'v4', 'retries': {'max_attempts': 10, 'mode': 'standard'}}, bucket='my-bucket', keys=['path/to/my/data_0.parquet', 'path/to/my/data_1.parquet', 'path/to/my/data_2.parquet'])\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "S3TrainingMetadata = namedtuple(\"S3TrainingMetadata\", \"config bucket keys\")\n",
    "\n",
    "training_metadata = S3TrainingMetadata(\n",
    "    s3_config, bucket_name,\n",
    "    [\"path/to/my/data_0.parquet\", \"path/to/my/data_1.parquet\", \"path/to/my/data_2.parquet\"]\n",
    ")\n",
    "\n",
    "experiment = project.log_experiment(training_metadata=training_metadata)\n",
    "print(training_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using projects for complex training metadata\n",
    "\n",
    "Each **experiment** on the *S3 Training Metadata* project below uses the same config to connect to S3, so no need to duplicate it. We'll only log it to the **project**. Then we'll run three experiments, with each one using a different key to load data from S3. We can represent that training metadata as a different `namedtuple` and log one to each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.914232Z",
     "iopub.status.busy": "2021-03-05T15:28:31.913186Z",
     "iopub.status.idle": "2021-03-05T15:28:31.915763Z",
     "shell.execute_reply": "2021-03-05T15:28:31.916274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3Config(region_name='us-west-2', signature_version='v4', retries={'max_attempts': 10, 'mode': 'standard'})\n",
      "S3DatasetMetadata(bucket='my-bucket', key='path/to/my/data_0.parquet')\n",
      "S3DatasetMetadata(bucket='my-bucket', key='path/to/my/data_1.parquet')\n",
      "S3DatasetMetadata(bucket='my-bucket', key='path/to/my/data_2.parquet')\n"
     ]
    }
   ],
   "source": [
    "S3Config = namedtuple(\"S3Config\", \"region_name signature_version retries\")\n",
    "S3DatasetMetadata = namedtuple(\"S3DatasetMetadata\", \"bucket key\")\n",
    "\n",
    "project = rubicon.get_or_create_project(\"S3 Training Metadata\", training_metadata=S3Config(**s3_config))\n",
    "\n",
    "print(project.training_metadata)\n",
    "\n",
    "for key in [\"path/to/my/data_0.parquet\", \"path/to/my/data_1.parquet\", \"path/to/my/data_2.parquet\"]:\n",
    "    experiment = project.log_experiment(\n",
    "        training_metadata=S3DatasetMetadata(bucket=bucket_name, key=key)\n",
    "    )\n",
    "    # then run the experiment and log everything to rubicon!\n",
    "    \n",
    "    print(experiment.training_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we can use the **project** and **experiments** to reconnect to the same datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T15:28:31.923477Z",
     "iopub.status.busy": "2021-03-05T15:28:31.922953Z",
     "iopub.status.idle": "2021-03-05T15:28:31.925398Z",
     "shell.execute_reply": "2021-03-05T15:28:31.925907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3Config(region_name='us-west-2', signature_version='v4', retries={'max_attempts': 10, 'mode': 'standard'})\n",
      "S3DatasetMetadata(bucket='my-bucket', key='path/to/my/data_2.parquet')\n",
      "S3DatasetMetadata(bucket='my-bucket', key='path/to/my/data_0.parquet')\n",
      "S3DatasetMetadata(bucket='my-bucket', key='path/to/my/data_1.parquet')\n"
     ]
    }
   ],
   "source": [
    "project = rubicon.get_project(\"S3 Training Metadata\")\n",
    "s3_config = S3Config(*project.training_metadata)\n",
    "\n",
    "print(s3_config)\n",
    "\n",
    "for experiment in project.experiments():\n",
    "    s3_dataset_metadata = S3DatasetMetadata(*experiment.training_metadata)\n",
    "    \n",
    "    print(s3_dataset_metadata)\n",
    "    \n",
    "    training_data = read_from_s3(\n",
    "        s3_config._asdict(), s3_dataset_metadata.bucket, s3_dataset_metadata.key, \"./data.parquet\"\n",
    "    )\n",
    "    # then review the experiment with a quick reference to the data it was ran against!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbsphinx": {
   "execute": "always"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
