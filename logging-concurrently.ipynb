{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Concurrently\n",
    "\n",
    "Once we've got the hang of using `rubicon`, we can expand on our project from the *Iris Classifier* example. \n",
    "Let's see how a few other popular `scikit-learn` models perform with the Iris dataset. `rubicon` logging is totally \n",
    "thread-safe, so we can test a lot of model configurations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:09.539287Z",
     "iopub.status.busy": "2021-03-01T18:47:09.538467Z",
     "iopub.status.idle": "2021-03-01T18:47:10.165813Z",
     "shell.execute_reply": "2021-03-01T18:47:10.165243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project(name='Concurrent Experiments', id='eeb659c2-58ed-481b-be9a-f4ebf0aac2fe', description='Training multiple models in parallel', github_url=None, training_metadata=None, created_at=datetime.datetime(2021, 3, 1, 18, 47, 10, 160020))\n"
     ]
    }
   ],
   "source": [
    "from rubicon import Rubicon\n",
    "\n",
    "root_dir = \"./rubicon-root\"\n",
    "\n",
    "rubicon = Rubicon(persistence=\"filesystem\", root_dir=root_dir)\n",
    "project = rubicon.create_project(\"Concurrent Experiments\", description=\"Training multiple models in parallel\")\n",
    "\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a recap of the contents of the Iris dataset, check out `iris.DESCR` and `iris.data`. We'll put together\n",
    "a training dataset using a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:10.170024Z",
     "iopub.status.busy": "2021-03-01T18:47:10.169532Z",
     "iopub.status.idle": "2021-03-01T18:47:10.578686Z",
     "shell.execute_reply": "2021-03-01T18:47:10.578145Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = int(datetime.utcnow().timestamp())\n",
    "\n",
    "iris = load_iris()\n",
    "iris_datasets = train_test_split(iris['data'], iris['target'], random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `run_experiment` to log a new experiment to the provided `project` then train, run and log a model of type\n",
    "`classifier_cls` using the training and testing data in `iris_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:10.586448Z",
     "iopub.status.busy": "2021-03-01T18:47:10.585235Z",
     "iopub.status.idle": "2021-03-01T18:47:10.587023Z",
     "shell.execute_reply": "2021-03-01T18:47:10.587488Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "SklearnTrainingMetadata = namedtuple(\"SklearnTrainingMetadata\", \"module_name method\")\n",
    "\n",
    "def run_experiment(project, classifier_cls, iris_datasets, **kwargs):\n",
    "    X_train, X_test, y_train, y_test = iris_datasets\n",
    "    \n",
    "    experiment = project.log_experiment(\n",
    "        training_metadata=[SklearnTrainingMetadata(\"sklearn.datasets\", \"load_iris\")],\n",
    "        model_name=classifier_cls.__name__,\n",
    "        tags=[classifier_cls.__name__],\n",
    "    )\n",
    "    \n",
    "    for key, value in kwargs.items():\n",
    "        experiment.log_parameter(key, value)\n",
    "    \n",
    "    n_features = len(iris.feature_names)\n",
    "    experiment.log_parameter(\"n_features\", n_features)\n",
    "    \n",
    "    for name in iris.feature_names:\n",
    "        experiment.log_feature(name)\n",
    "        \n",
    "    classifier = classifier_cls(**kwargs)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    classifier.predict(X_test)\n",
    "    \n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    \n",
    "    experiment.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    if accuracy >= .95:\n",
    "        experiment.add_tags([\"success\"])\n",
    "    else:\n",
    "        experiment.add_tags([\"failure\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll take a look at two more classifiers, **decision tree** and **k-neighbors**, in addition to the **random forest** classifier we used in the last example. Each classifier will be ran across four sets of parameters (provided as `kwargs` to `run_experiment`), for a total of 12 experiments. Here, we'll build up a list of processes that will run each experiment in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:10.593265Z",
     "iopub.status.busy": "2021-03-01T18:47:10.592770Z",
     "iopub.status.idle": "2021-03-01T18:47:10.634426Z",
     "shell.execute_reply": "2021-03-01T18:47:10.633988Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "processes = []\n",
    "\n",
    "for n_estimators in [10, 20, 30, 40]:\n",
    "    processes.append(multiprocessing.Process(\n",
    "        target=run_experiment, args=[project, RandomForestClassifier, iris_datasets],\n",
    "        kwargs={\"n_estimators\": n_estimators, \"random_state\": random_state},\n",
    "    ))\n",
    "    \n",
    "for criterion in [\"gini\", \"entropy\"]:\n",
    "    for splitter in [\"best\", \"random\"]:\n",
    "        processes.append(multiprocessing.Process(\n",
    "            target=run_experiment, args=[project, DecisionTreeClassifier, iris_datasets],\n",
    "            kwargs={\"criterion\": criterion, \"splitter\": splitter, \"random_state\": random_state},\n",
    "        ))\n",
    "\n",
    "for n_neighbors in [5, 10, 15, 20]:\n",
    "    processes.append(multiprocessing.Process(\n",
    "        target=run_experiment, args=[project, KNeighborsClassifier, iris_datasets],\n",
    "        kwargs={\"n_neighbors\": n_neighbors},\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run all our experiments in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:10.638144Z",
     "iopub.status.busy": "2021-03-01T18:47:10.637552Z",
     "iopub.status.idle": "2021-03-01T18:47:10.847722Z",
     "shell.execute_reply": "2021-03-01T18:47:10.848237Z"
    }
   },
   "outputs": [],
   "source": [
    "for process in processes:\n",
    "    process.start()\n",
    "    \n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can validate that we successfully logged all 12 experiments to our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:10.852291Z",
     "iopub.status.busy": "2021-03-01T18:47:10.851807Z",
     "iopub.status.idle": "2021-03-01T18:47:10.865009Z",
     "shell.execute_reply": "2021-03-01T18:47:10.864596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(project.experiments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which experiments we tagged as successful and what type of model they used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:10.868553Z",
     "iopub.status.busy": "2021-03-01T18:47:10.867984Z",
     "iopub.status.idle": "2021-03-01T18:47:10.881571Z",
     "shell.execute_reply": "2021-03-01T18:47:10.881091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment 39debc41-30cc-40ca-b969-d05cf1c62409 was successful using a RandomForestClassifier\n",
      "experiment 488e1f60-5fc6-40fe-91ea-3b26d8745186 was successful using a KNeighborsClassifier\n",
      "experiment 5436544d-791f-4dcf-8e27-5602eecd56f6 was successful using a KNeighborsClassifier\n",
      "experiment 67248252-2e61-410e-81f5-aebb64783424 was successful using a RandomForestClassifier\n",
      "experiment 6ab60b0b-4a2a-421c-8d4b-4f7dc31b6011 was successful using a RandomForestClassifier\n",
      "experiment caa8f632-b407-4b40-a9a4-245633b809d3 was successful using a RandomForestClassifier\n"
     ]
    }
   ],
   "source": [
    "for e in project.experiments(tags=[\"success\"]):    \n",
    "    print(f\"experiment {e.id} was successful using a {e.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a deeper look at any of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T18:47:10.885670Z",
     "iopub.status.busy": "2021-03-01T18:47:10.885187Z",
     "iopub.status.idle": "2021-03-01T18:47:10.896006Z",
     "shell.execute_reply": "2021-03-01T18:47:10.896757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_metadata: SklearnTrainingMetadata(module_name='sklearn.datasets', method='load_iris')\n",
      "tags: ['failure', 'KNeighborsClassifier']\n",
      "parameters:\n",
      "\tn_features: 4\n",
      "\tn_neighbors: 20\n",
      "features:\n",
      "\tpetal length (cm)\n",
      "\tpetal width (cm)\n",
      "\tsepal length (cm)\n",
      "\tsepal width (cm)\n",
      "metrics:\n",
      "\taccuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "experiment = project.experiments()[0]\n",
    "\n",
    "print(f\"training_metadata: {SklearnTrainingMetadata(*experiment.training_metadata)}\")\n",
    "print(f\"tags: {experiment.tags}\")\n",
    "print(\"parameters:\")\n",
    "for parameter in experiment.parameters():\n",
    "    print(f\"\\t{parameter.name}: {parameter.value}\")\n",
    "print(\"features:\")\n",
    "for feature in experiment.features():\n",
    "    print(f\"\\t{feature.name}\")\n",
    "print(\"metrics:\")\n",
    "for metric in experiment.metrics():\n",
    "    print(f\"\\t{metric.name}: {metric.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model developers can take advantage of `rubicon`'s thread-safety to test tons of models at once and collect results\n",
    "in a standardized format to analyze which ones performed the best. `rubicon` can even be run in more advanced\n",
    "distributed setups, like on a *Dask* cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbsphinx": {
   "execute": "always"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
